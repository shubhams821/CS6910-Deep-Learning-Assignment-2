# -*- coding: utf-8 -*-
"""Part_A.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5hvUvloc6TgPC-ROPihGd_hbDzAVIGF
"""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from torchvision import transforms
import wandb
import pytorch_lightning as pl
from torchvision import transforms
import os
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
from pytorch_lightning.loggers import WandbLogger

# Define data transformations
data_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Define the paths to the training and validation directories
train_dir = '/content/drive/MyDrive/Assignment 2/inaturalist_12K/train'         # Add appropiate path
val_dir = '/content/drive/MyDrive/Assignment 2/inaturalist_12K/val'

# Create the training and validation datasets
train_dataset = ImageFolder(train_dir, transform=data_transforms)
val_dataset = ImageFolder(val_dir, transform=data_transforms)

# Create the data loaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)
val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)

wandb.login()

class Mish(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x * torch.tanh(F.softplus(x))

class CNN(pl.LightningModule):
    def __init__(self, input_shape, num_classes, num_filters=32, filter_size=(3, 3), activation='relu',
                 dense_neurons=128, dropout_rate=0.2, data_augmentation=False, batch_norm=True):  # Added batch_norm parameter
        super().__init__()
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.num_filters = num_filters
        self.filter_size = filter_size
        self.activation = activation
        self.dense_neurons = dense_neurons
        self.dropout_rate = dropout_rate
        self.data_augmentation = data_augmentation
        self.batch_norm = batch_norm  # New parameter

        # Define the layers
        self.conv_layers = self._create_conv_layers()
        self.dense_layers = self._create_dense_layers()

    def _create_conv_layers(self):
        layers = []
        in_channels = self.input_shape[0]
        for out_channels in [self.num_filters, self.num_filters, self.num_filters, self.num_filters, self.num_filters]:
            layers.append(nn.Conv2d(in_channels, out_channels, self.filter_size, padding='same'))
            if self.batch_norm:
                layers.append(nn.BatchNorm2d(out_channels))
            layers.append(self.get_activation())
            layers.append(nn.MaxPool2d(2))
            in_channels = out_channels
        return nn.Sequential(*layers)

    def _create_dense_layers(self):
        layers = [
            nn.Flatten(),
            nn.Linear(self.conv_output_size(), self.dense_neurons),
            self.get_activation(),
            nn.Dropout(self.dropout_rate),
            nn.Linear(self.dense_neurons, self.num_classes)
        ]
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv_layers(x)
        x = self.dense_layers(x)
        return x

    def get_activation(self):
        if self.activation == 'relu':
            return nn.ReLU()
        elif self.activation == 'gelu':
            return nn.GELU()
        elif self.activation == 'silu':
            return nn.SiLU()
        elif self.activation == 'mish':
            return Mish()  # Assuming you have implemented the Mish activation function
        else:
            raise ValueError("Invalid activation function specified.")

    def conv_output_size(self):
        dummy_input = torch.zeros(1, *self.input_shape)
        dummy_output = self.conv_layers(dummy_input)
        return dummy_output.flatten(1).size(-1)

    def training_step(self, batch, batch_idx):
        images, labels = batch
        outputs = self(images)
        loss = F.cross_entropy(outputs, labels)
        accuracy = (outputs.argmax(dim=1) == labels).float().mean()
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.log('train_accuracy', accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        # wandb.log({ "train_acc": accuracy, "train_loss": loss, "epoch": epoch+1})
        return loss

    def validation_step(self, batch, batch_idx):
        images, labels = batch
        outputs = self(images)
        loss = F.cross_entropy(outputs, labels)
        accuracy = (outputs.argmax(dim=1) == labels).float().mean()
        self.log('train_loss', loss, on_epoch=True, prog_bar=True, logger=True)
        self.log('train_accuracy', accuracy, on_epoch=True, prog_bar=True, logger=True)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

sweep_config = {
    'method': 'random',  # Specify the search method
    'parameters': {
        'num_filters': {'values': [32, 64, 128]},
        'activation': {'values': ['relu', 'gelu', 'silu', 'mish']},
        'data_augmentation': {'values': [True, False]},
        'batch_norm': {'values': [True, False]},
        'dropout': {'values': [0.2, 0.3]},
        'filter_organization': {'values': ['same', 'double', 'halve']}
    }
}


sweep_id = wandb.sweep(sweep=sweep_config, project="DL_Assignment_2_try_1")

def train_model():
    with wandb.init() as run:

        run_name = f"num_filters_{wandb.config.num_filters}_activation_{wandb.config.activation}_data_augmentation_{wandb.config.data_augmentation}_batch_norm_{wandb.config.batch_norm}_dropout_{wandb.config.dropout}_filter_organization_{wandb.config.filter_organization}"
        wandb.run.name = run_name
        config = wandb.config

        # Create the training and validation datasets
        train_dataset = ImageFolder(train_dir, transform=data_transforms)
        val_dataset = ImageFolder(val_dir, transform=data_transforms)

        # Create the data loaders
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)
        input_shape = (3, 224, 224)
        num_classes = len(train_dataset.classes)
        model = CNN(input_shape, num_classes, num_filters=config.num_filters, activation=config.activation,
                    data_augmentation=config.data_augmentation, batch_norm=config.batch_norm,
                    dropout_rate=config.dropout, filter_organization=config.filter_organization)
        model = model.to('cuda')

        # Initialize wandb logger
        wandb_logger = WandbLogger()
        # Train the model
        trainer = pl.Trainer(max_epochs=10,accelerator='gpu', devices=1, logger=wandb_logger)
        trainer.fit(model, train_loader, val_loader)

wandb.agent(sweep_id, function=train_model, count = 50)